{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678a8cb6",
   "metadata": {},
   "source": [
    "## 6. Decision Trees and Ensemble Learning\n",
    "\n",
    "This week, we'll talk about decision trees and tree-based ensemble algorithms\n",
    "\n",
    "### 6.1 Credit risk scoring project\n",
    "\n",
    "Dataset: [https://github.com/gastonstat/CreditScoring](https://github.com/gastonstat/CreditScoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b86b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454b01",
   "metadata": {},
   "source": [
    "### 6.2 Data cleaning and preparation\n",
    "\n",
    "* Downloading the dataset\n",
    "* Re-encoding the categorical variables\n",
    "* Doing the train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b126d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef6a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget $data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d039e",
   "metadata": {},
   "source": [
    "### 6.3 Decision trees\n",
    "\n",
    "* How a decision tree looks like\n",
    "* Training a decision tree\n",
    "* Overfitting\n",
    "* Controlling the size of a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e43c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc477165",
   "metadata": {},
   "source": [
    "### 6.4 Decision tree learning algorithm\n",
    "\n",
    "* Finding the best split for one column\n",
    "* Finding the best split for the entire dataset\n",
    "* Stopping criteria\n",
    "* Decision tree learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cf55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8880b278",
   "metadata": {},
   "source": [
    "### 6.5 Decision trees parameter tuning\n",
    "\n",
    "* selecting `max_depth`\n",
    "* selecting `min_samples_leaf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0eaccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8503306b",
   "metadata": {},
   "source": [
    "### 6.6 Ensembles and random forest\n",
    "\n",
    "* Board of experts\n",
    "* Ensembling models\n",
    "* Random forest - ensembling decision trees\n",
    "* Tuning random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090894eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea275d4",
   "metadata": {},
   "source": [
    "Other useful parametes:\n",
    "\n",
    "* `max_features`\n",
    "* `bootstrap`\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "### 6.7 Gradient boosting and XGBoost\n",
    "\n",
    "* Gradient boosting vs random forest\n",
    "* Installing XGBoost\n",
    "* Training the first model\n",
    "* Performance monitoring\n",
    "* Parsing xgboost's monitoring output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26a2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4e8bee",
   "metadata": {},
   "source": [
    "### 6.8 XGBoost parameter tuning\n",
    "\n",
    "Tuning the following parameters:\n",
    "\n",
    "* `eta`\n",
    "* `max_depth`\n",
    "* `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8816b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0429c5d",
   "metadata": {},
   "source": [
    "### 6.9 Selecting the final model\n",
    "\n",
    "* Choosing between xgboost, random forest and decision tree\n",
    "* Training the final model\n",
    "* Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b13062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d499ec9",
   "metadata": {},
   "source": [
    "### 6.10 Summary\n",
    "\n",
    "* Decision trees learn if-then-else rules from data.\n",
    "* Finding the best split: select the least impure split. This algorithm can overfit, that's why we control it by limiting the max depth and the size of the group.\n",
    "* Random forest is a way of combininig multiple decision trees. It should have a diverse set of models to make good predictions.\n",
    "* Gradient boosting trains model sequentially: each model tries to fix errors of the previous model. XGBoost is an implementation of gradient boosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439467ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f77b6cb",
   "metadata": {},
   "source": [
    "### 6.11 Explore more\n",
    "\n",
    "* For this dataset we didn't do `EDA` or feature engineering. You can do it to get more insights into the problem.\n",
    "* For random forest, there are more parameters that we can tune. Check `max_features` and `bootstrap`.\n",
    "* There's a variation of random forest caled \"extremely randomized trees\", or \"extra trees\". Instead of selecting the best split among all possible thresholds, it selects a few thresholds randomly and picks the best one among them. Because of that extra trees never overfit. In Scikit-Learn, they are implemented in `ExtraTreesClassifier`. Try it for this project.\n",
    "* XGBoost can deal with NAs - we don't have to do `fillna` for it. Check if not filling NA's help improve performance.\n",
    "* Experiment with other XGBoost parameters: `subsample` and `colsample_bytree`.\n",
    "* When selecting the best split, decision trees find the most useful features. This information can be used for understanding which features are more important than otheres. See example here for [random forest](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) (it's the same for plain decision trees) and for [xgboost](https://stackoverflow.com/questions/37627923/how-to-get-feature-importance-in-xgboost)\n",
    "* Trees can also be used for solving the regression problems: check `DecisionTreeRegressor`, `RandomForestRegressor` and the `objective=reg:squarederror` parameter for XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942d715",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
